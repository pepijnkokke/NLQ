\section{Introduction}
\label{sec:introduction}

In this thesis, I will discuss the grammar logic NLQ, an extension of
the non-associative Lambek calculus, which is capable of analysing
quantifier movement, scope islands, infixation and extraction.

% This thesis is not meant to be a work of thorough linguistic
% analysis.
What I hope to do in this thesis is to \emph{extend} and
\emph{solidify} the logical vocabulary with which such linguistic
analyses can be made. In this, I will use the following guiding
principles:
\begin{itemize}
\item We are constructing a \emph{grammar} logic. Therefore, we only
  want features in our logic for which we can demonstrate a motivating
  example from natural language.
\item We are constructing a grammar \emph{logic}. Therefore, we will
  only accept extensions to our logic if we can show that they
  preserve our most important properties: we want our logic to be
  reflexive and transitive, and want a procedure for proof
  search that is both \emph{decidable} and \emph{complete}.
\end{itemize}
I am under no impression that the extensions I am proposing will be
the be-all and end-all of logical grammar, so another important point
in this thesis will be \emph{modularity}.
It is incredibly important to formulate extensions in a modular
manner, so that other logical grammarians are free to mix and
match extensions without having to worry about unforeseen
interactions. There are two key techniques for this:
\begin{enumerate*}[label=(\arabic*)]
\item
  we use display calculus to get a general procedure for
  cut-elimination; and
\item
  we associate each syntactic extension with its own set of
  connectives (or \emph{modality}) and make sure that the inference
  rules in that extension only apply in the presence of these
  connectives.
\end{enumerate*}

\todo{Key point: we need a normal-form for our calculus.}

The last key point in this thesis will be \emph{verification}. It is
far too easy to make mistakes when writing down logical proofs in a
pen-and-paper style, or when manually typesetting them in
\LaTeX. Therefore, most of the claims I make in this thesis will be
backed up by a pair of verified implementations of the full version
of NLQ (i.e. the version using all discussed extensions).
These verifications can be found in appendices A and B and on GitHub.

In appendix A, we discuss a formalisation in Agda \citep{norell2009}.
We implement the grammar logic, prove some key properties, and give a
formal semantics in the form of a translation from proofs in NLQ to
Agda terms.

In appendix B, we discuss a formalisation in Haskell
\citep{marlow2010} using the singletons library
\citet{eisenberg2012}. In the full version, we implement the grammar
logic, implement proof search, and give a formal semantics in the form
of a translation into a subset of Haskell which includes meaning
postulates. However, because the implementations of the grammar logic
are nearly identical, we restrict our discussion of the Haskell
version to the interface provided by the library, and how to write
your own lexicon and example sentences.

Starting in \autoref{sec:what-is-type-logical-grammar}, we will give a
brief introduction to type-logical grammar in general, and to what I
consider to be the base type-logical grammar: the non-associative
Lambek calculus (NL) paired with a simple semantic lambda calculus
(\lamET).
In \autoref{sec:display-calculus}, we will discuss the display
calculus formulation of NL, and motivate our usage of display
calculus.
Then, in sections \ref{sec:lexical-ambiguity} and
\ref{sec:movement-and-quantifier-raising}, we will discuss several
extension to the base type-logical grammar.

\subsection{What is type-logical grammar?}
\label{sec:what-is-type-logical-grammar}

Before we address the question of what type-logical grammar is, let us
try and get an idea of what problem it is trying to solve. Have a look
at the abstract pipeline for natural language understanding (NLU) in
\autoref{fig:abstract-nlu-pipeline}.

\input{fig-abstract-nlu-pipeline}

To the left of the figure, you see the various phases or functions
commonly associated with an NLU-pipeline. To the right, you see the
inputs and outputs of these functions.
For instance, the morphological function will take an unanalysed
sentence, and return a sentence which is lemmatised. This entails that
all morphemes are made explicit---for instance, in the case of the
example in \autoref{fig:abstract-nlu-pipeline}, the previously
``implicit'' morphemes for past tense and plurality are added.

There is some disagreement on the exact role of type-logical grammars in
this pipeline. Ideally, type-logical grammars would play the role of
both the syntactic and the semantic function. However, the current
state of affairs in research is that often only the semantic function
is truly considered\footnote{%
  This statement is not true for \emph{associative} type-logical
  grammars, which fundamentally reject the tree structure of
  language---that is, they assume that the meaning of a sentence
  depends solely on the linear order of words, and not on some hidden
  tree structure.
}. This makes sense from a research perspective: we can refer to the
huge body of work on generative grammar to inform our choice for
sentence structure, and focus on assigning the right meaning to these
structures. This is also the approach we will also take in this
thesis---that is, we consider type-logical grammars to be the
function:
\begin{center}
  Mary:NP [see:TV.PAST fox:NP.PL]\\
  $\downarrow$\\
  \framebox{Type-Logical Grammar}\\
  $\downarrow$\\
  $\exists X.X\subseteq\mathbf{fox}\land\mathbf{past}(\mathbf{see}(\text{mary},X))$
\end{center}
That is, it is a function which, given some structured and typed input
which represents the syntactic structure of a sentence, returns the
meaning(s) associated with that sentence.

Given the presence of the phrase ``structured and typed'', we may
already suspect that type theory offers a fitting solution to this
problem. And indeed, under the guise of type-logical grammar, it
does. A type-logical grammar generally consists of three things:
\begin{enumerate}[label=(\arabic*)]
\item a syntactic calculus, set up in such a fashion that only
  grammatical sentences are well-typed, and for which an efficiently
  decidable procedure for proof-search exists;
\item a semantic calculus, used to represent the meanings of words and
  sentences; and
\item a translation from the syntactic to the semantic calculus.
\end{enumerate}
We interpret the part-of-speech tags in our input (NP, TV, etc.) as
types in the syntactic calculus, and combine these with the desired
type for the tree---usually \S\ for `sentence'---to form an input
sequent.
We then search for a proof of that sequent in the syntactic calculus,
and translate it to a term in the semantic calculus.
Once there, we interpret the morphemes (e.g.\ lemmas, \texttt{PAST},
\texttt{PL}, etc.) as terms in the semantic calculus.

In \autoref{sec:simple-type-logical-grammar}, we will have a look at
the base type-logical grammar, and give some examples of the process
of deriving sentence meaning.



\subsection{A simple type-logical grammar}
\label{sec:simple-type-logical-grammar}

The simplest type-logical grammar that comes to mind---drawing
heavily from Montague grammar and categorial grammar---is composed of
the simply-typed lambda calculus with atomic types \e\ and \t\ (\lamET) as
a semantic calculus, and the non-associative Lambek calculus
\citep[NL;][]{lambek1961} as a syntactic calculus.

The usual natural deduction formulation of \lamET\ can to be seen in
\autoref{fig:implicit-lamET}. It is a simple lambda
calculus, with atomic types \e\ (`entity') and \t\ (`truth-value').
In addition, we usually assume that any logical operator or
word-meanings we need is defined as a constant of the appropriate
type. For instance, $\forall$ is a constant of type $(\e\ra\t)\ra\t$,
and `john' is a constant of type \e. Note that we will sometimes write
logical operators in their usual notation, e.g.\ $M\wedge N$ or
$\forall x.M$, but this should be taken as syntactic sugar, in the
case of our examples rewriting to $(({\wedge}\;M)\;N)$ and
$\forall\;(\lambda{x}.M)$, respectively. Additionally, we will
occasionally write e.g.\ $\e\e\t$ instead of $\e\ra\e\ra\t$, or
$(\e\t)\t$ instead of $(\e\ra\t)\ra\t$, using adjacency to mean
implication.

\input{fig-implicit-lamET}%

Using this calculus as a semantics function directly would
over-generate, e.g.\ for the sequent $\{\text{john}:\e,
\text{likes}:\e\ra\e\ra\t, \text{mary}:\e\}\fCenter\t$ we can derive
$((\text{likes}\; \text{john})\; \text{mary})$, $((\text{likes}\;
\text{mary})\; \text{john})$, $((\text{likes}\; \text{mary})\;
\text{mary})$ and $((\text{likes}\; \text{john})\; \text{john})$.
The reason for this is, of course, that the set structure used in this
formulation is much too expressive for natural language grammar.

If we want more control over the structure of our terms, a good first
step is to move to a purely syntactic formulation, where all the
structural properties are made explicit in the calculus itself; this
has been done in \autoref{fig:explicit-lamET}. We have
replaced the set by a (possibly empty) binary tree, spanned by the
structural product `$\prod$'. We have also included a number of new
structural rules, which implement the structure of a set: $\emptyset$E
and $\emptyset$I allow us to have an empty antecedent; contraction and
weakening tell us that we can use formulas multiple times or not at
all; and with commutativity and associativity we can change the order
of the formulas any way we like.

\input{fig-explicit-lamET}

Note that, in order to define these structural rules, we had to define
the notion of a `context'---a structure with \emph{exactly one} hole
in it---and a plugging function `\plug'---a function which inserts a
structure into that hole. The reason for this is that we have to be
able to apply commutativity and associativity \emph{anywhere} in the
structure to be able to freely change the order (and
bracketing).\footnote{%
  The contexts are not strictly necessary for $\emptyset$E,
  contraction and weakening, since we can already move any formula
  anywhere we want, but they make the proof system much more usable
  and greatly decrease the length of proofs that need to use any of
  these structural rules.
}

It is not hard to convince yourself that the implicit and explicit
versions of \lamET\ are equivalent---though we will refrain from
giving the full proof here.
Because of this equivalence, we can use the term language from
\autoref{fig:implicit-lamET} for the explicit version of
\lamET.
The term labelling of the logical rules is exactly the same. The
structural rules only manipulate structures, and therefore do not
change the terms. The only exception to this is contraction, for which
the term labelling is as follows:
\begin{prooftree}
  \AXC{$Σ[y : A\prod z : A]\fCenter M : B$}
  \RightLabel{Cont.}
  \UIC{$Σ[x : A]\fCenter M[x/y][x/z] : B$}
\end{prooftree}
Contraction takes a term with two variables of the same type, and
contracts them using substitution, which is defined as usual:
\begin{alignat*}{3}
  &x             &&[N/y] \mapsto
  \begin{cases}
    N, &\text{if}\;x=y\\
    x, &\text{otherwise}
  \end{cases}
  \\
  &C             &&[N/y] \mapsto C\\
  &(\lambda x.M) &&[N/y] \mapsto
  \begin{cases}
    \lambda x.M[N/y], &\text{if}\;x=y\\
    \lambda x.M,      &\text{otherwise}
  \end{cases}
  \\
  &(M\;M')       &&[N/y] \mapsto (M[N/y]\;M'[N/y])
\end{alignat*}

Using our explicit semantic calculus, we can construct our syntactic
calculus in three simple steps:
\begin{enumerate}
\item%
  we drop \emph{all} structural rules;
\item%
  since the implication `$\ra$' can now only take arguments directly
  from the left, we add a second implication `$\la$' which can only
  take arguments from the right---by convention, implications in this
  system are written as `$\impr$' and `$\impl$' (pronounced ``under''
  and ``over'') with the argument type written \emph{under} the slash;
\item%
  we replace the atomic semantic types \e\ and \t\ by atomic syntactic
  types, reminiscent of part-of-speech tags---in this case, we will
  use S (`sentence'), NP (`noun phrase'), N (`noun'), PP
  (`prepositional phrase') and INF (`infinitive');
\end{enumerate}
The resulting system can be seen in \autoref{fig:nl-natural-deduction},
defined along with some definitions for common part-of-speech tags,
i.e.\ \A\ (`adjective'), \IV\ (`intransitive verb') and \TV\
(`transitive verb').

\input{fig-nl-natural-deduction}

Dropping \emph{all} structural rules may seem unnecessary, but there
is a good motivation for each rule.  For example, in the presence of
commutativity, there is no way to distinguish between ``Mary walks''
and ``walks Mary''; under weakening, we can add any word anywhere in a
grammatical sentence, and the sentence will remain grammatical---
e.g.\ ``Mary banana walks''; and with contraction, we can remove
consecutive words with the same type---which means that ``John read
a fantastic blue book'' could be taken to mean the same thing as
``John read a blue book''.

With respect to associativity, \citet[][p.\ 167]{lambek1961} mentions
that ``the most natural assignments of types to English words [would]
admit many pseudo-sentences as grammatical, e.g.\ %
\begin{center}
  (*)~\itshape John is poor sad. John likes poor him. Who works and
  John rests?
\end{center}
More examples, including specific derivations, of ungrammatical
sentences that would be admitted in the presence of associativity and
the empty structure can be found in \citet[p.\ 33, 105-106]{moot2012}.

Note that we use the product-free version of NL. The reason for this
is that we have no use for the product in this thesis. Should you need
the product, however, it is very easily added:
\begin{center}
  \begin{pfbox}
    \AXC{$\Gamma\fCenter{A}$}
    \AXC{$\Delta\fCenter{B}$}
    \RightLabel{L$\otimes$}
    \BIC{$\Gamma\prod\Delta\fCenter{A\otimes{B}}$}
  \end{pfbox}
  \begin{pfbox}
    \AXC{$\Gamma\fCenter{A\otimes{B}}$}
    \AXC{$\Sigma[A\prod{B}]\fCenter{C}$}
    \RightLabel{R$\otimes$}
    \BIC{$\Sigma[\Gamma]\fCenter{C}$}
  \end{pfbox}
\end{center}

The last component we need for our simple type-logical grammar is a
translation from our syntactic calculus to our semantic calculus,
which consists of:
\begin{enumerate}[label=(\arabic*)]
\item
  a function $\tr$, translating the types in NL to types in \lamET; and
\item
  a set of rewrite rules, that rewrite proofs in NL to proofs in \lamET.
\end{enumerate}
However, in the interest of brevity, we will often give this second
translation directly as a term labelling. For instance, in
\autoref{fig:nl-natural-deduction-to-lamET}, we give the
translation on terms by directly labelling the rules of the syntactic
calculus with semantic terms. Because there is a one-to-one
correspondence between lambda terms and proofs, this is perfectly
unambiguous.

Note that we have chosen the particular translation for atomic types
in \autoref{fig:nl-natural-deduction-to-lamET} because it
aligns well with the remainder of this thesis. However, there are
different ways to define this translation---most notably,
\citepos{montague1973} worst-case generalisation for NPs, which
interprets them as having the type $(\e\t)\t$.
\todo{Reference to the correct work by Montague?}

Now that we have a full type-logical grammar, let's give an example
analysis of the sentence ``Mary likes Bill''. We assume the
morphological, lexical and syntactic phases have been taken care of,
which leaves us with the following endsequent:
\[
  \text{mary}:\NP\prod(\text{likes}:\TV\prod\text{bill}:\NP)\;\fCenter\;?:\S
\]
Fortunately, proof search is decidable for this system, so we can
simply search the space of all possible proofs of this sequent. As it
turns out, the only proof is:
\begin{center}
  \vspace*{-1\baselineskip}
  \begin{pfbox}[0.8]
    \AXC{}\RightLabel{Ax}\UIC{$\text{mary}:\NP\fCenter\text{mary}:\NP$}
    \AXC{}\RightLabel{Ax}\UIC{$\text{likes}:\TV\fCenter\text{likes}:(\NP\impr\S)\impl\NP$}
    \AXC{}\RightLabel{Ax}\UIC{$\text{bill}:\NP\fCenter\text{bill}:\NP$}
    \RightLabel{$\impl$E}
    \BIC{$\text{likes}:\TV\prod\text{bill}:\NP\fCenter(\text{likes}\;\text{bill}):\NP\impr\S$}
    \RightLabel{$\impr$E}
    \BIC{$\text{mary}:\NP\prod(\text{likes}:\TV\prod\text{bill}:\NP)\fCenter((\text{likes}\;\text{bill})\;\text{mary}):\S$}
  \end{pfbox}
\end{center}
And so, by searching for a proof in our syntactic calculus (bottom-up)
and then adding in the term labelling (top-down) we derive a
function-argument structure for our sentence. Usually, we include
another step in this process, where we insert the lexical definitions
for the words. For the above example, these are:\footnote{%
  We use bold-face to distinguish between the variables associated
  with each word, and the meaning postulates we use in our semantics.
}
\[
  \begin{aligned}
    &\text{mary}  &&= \MARY\\
    &\text{john}  &&= \JOHN\\
    &\text{likes} &&= \lambda{y}.\lambda{x}.\LIKE(x,y)
  \end{aligned}
\]
After inserting these definitions, and $\beta$-reducing, we get:
\[
  \LIKE(\JOHN,\MARY)
\]
Because we are usually only interested in the resulting
function-argument structure and the associated semantics, for the
remainder of this thesis we will summarise the above translations as
follows:
\begin{center}
  \begin{pfbox}
    \AXC{}\RightLabel{Ax}\UIC{$\NP\fCenter\NP$}
    \AXC{}\RightLabel{Ax}\UIC{$\TV\fCenter(\NP\impr\S)\impl\NP$}
    \AXC{}\RightLabel{Ax}\UIC{$\NP\fCenter\NP$}
    \RightLabel{$\impl$E}
    \BIC{$\TV\prod\NP\fCenter(\NP\impr\S$}
    \RightLabel{$\impr$E}
    \BIC{$\NP\prod(\TV\prod\NP)\fCenter:\S$}
  \end{pfbox}
  \vspace*{-1\baselineskip}
  \begin{gather*}
    \downmapsto
    \\
    ((\text{likes}\;\text{bill})\;\text{mary})
    \\
    \downmapsto
    \\
    \LIKE(\JOHN,\MARY)
  \end{gather*}
\end{center}


\subsection{Sequent calculus and proof search}
\label{sec:sequent-calculus-and-proof-search}

In the previous section, we glossed over the issue of proof
search. This is problematic, because the natural deduction formulation
of the syntactic calculus we presented in
\autoref{fig:nl-natural-deduction} is not especially suited to proof
search. \citeauthor{lambek1961} originally developed a sequent
calculus for NL, which \emph{does} have a practical procedure for
proof search. In \autoref{fig:nl-sequent-calculus} we present the
product-free version of \citepos{lambek1961} sequent calculus.

\input{fig-nl-sequent-calculus}

One important property of sequent calculus, is the \emph{sub-formula}
property---the property that a derivation of a sequent uses only
proper sub-formulas of the formulas in that sequent.
As a direct consequence of this property, we generally get an
algorithm for proof search which is both easy to implement, and
complete. This algorithm is backward-chaining proof search: we
\begin{enumerate*}[label=(\arabic*)]
\item start with the desired sequent;
\item branch, applying each rule that can be applied; and
\item repeat.
\end{enumerate*}
This algorithm is trivially complete, because we try all rules. It is
also trivially guaranteed to terminate, since a derivation can only
use sub-formulas of the formulas in the conclusion---at each
successive step, the number of available formulas becomes strictly
smaller, and so we will eventually run out of formulas.

The sequent calculus formulation is equivalent to the natural
deduction formulation from \autoref{fig:nl-natural-deduction}.
This is trivial to prove once you have a procedure for cut-elimination
\citep[see][p.\ 107]{moot2012}. Therefore, we are still able to
translate to \lamET, and obtain an interpretation. However, in the
next section we will discuss the alternative to this sequent calculus
that we will use, so we will forgo this exercise.
