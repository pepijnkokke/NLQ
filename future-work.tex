\section{Related and Future work}
\label{sec:future-work}



\subsection*{Integrate Focusing and Display Logic}
In \autoref{sec:focusing-and-spurious-ambiguity} we mentioned that, at
the moment, there is no work which integrates focusing and display
logic. Although we have a proof of cut-elimination for display NL and
LG, due to \citet{bastenhof2011}, we have not extended this proof to
fully cover \NLBS.
As mentioned in \autoref{sec:focusing-and-spurious-ambiguity}, the
reason for this is that in doing so, we would undo the advantage of
using display calculus: if we have to maintain the proof of
cut-elimination ourselves, then why use display calculus?

However, for the small number of natural language examples that were
presented in this thesis, and those that were analysed in the Haskell
implementation, focusing does exactly as advertised: it greatly
reduces spurious ambiguity---eliminating it in many cases---while
retaining all useful ambiguity.
Therefore, it would be interesting to see the focusing integrated with
display calculus, so that we would have a principled approach to
developing focused display calculi.



\subsection*{Deep Inference Sequent Calculus}
In \autoref{sec:why-use-display-calculus}, we discussed that in order
to use backward-chaining proof search, we have to ensure that our
structural rules have the sub-structure property, or at very least
only cause predictable loops. \citet{gore2014} demonstrate a
methodology for constructing a deep inference sequent calculus from a
display calculus. Deep inference calculi have the advantage that they
naturally have the sub-structure property, which means that they are
suitable for naive backward-chaining search. It would be interesting
to employ this methodology, and construct a deep inference sequent
calculus for the system constructed in this thesis.



\subsection*{Forward-Chaining Proof Search}
In \autoref{sec:what-is-type-logical-grammar} it was mentioned that
most research focuses on implementing what I call the `semantic
function' (i.e.\ interpreting), and that we use backward-chaining
proof search mostly because it is a pleasant tool for research
purposes:
it allows us to look to the huge body of work on generative grammar to
inform our choices in parse trees, and focus our efforts on
associating the right meanings to these known structures.
However, in order to be feasible in a practical system, one must also
implement what I call the `syntactic function' (i.e.\ parsing).

The naive way to implement such a parsing algorithm, is to simply
enumerate all the possible structures for a given sentence, and try
them all. In practical parsing, however, this is not an option. The
number of binary syntax trees for a sentence of $n$ words is equal to
the $n$th Catalan number.

A realistic way to implement parsing is by moving away from our
backward-chaining proof search, and using forward-chaining proof
search. For a naive implementation of this, we create a bag of
axioms---one for each word---and construct all possible proofs that we
can construct with only this set of axioms. Then we filter on those
which are both pronounceable and maintain the correct
word-order. Ideally, however, we would have an efficient
implementation, for instance based on the technique of magic sets as
developed by \citet{bancilhon1985}.

In \autoref{sec:scope-islands} we proposed to analyse scope islands
with the unary diamond ($\di$), which, in certain situations, enforces
the presence of a structural diamond ($\langle\ldots\rangle$).
It may be troubling to some of the readers that in order to deal with
scope islands, we require that a structural connective is present in
our input, in the endsequent.
If we use forward-chaining proof search, however, this is much less
problematic than it seems. When using forward-chaining search to look
for all proofs of e.g.\ ``Mary said everyone left'', the logical
diamond in the type of `said' will naturally ensure that the
structural diamond is put into place, since they will be introduced
symmetrically.


\subsection*{Integrate with Effectful Semantics}
In sections \ref{sec:lexical-ambiguity} and
\ref{sec:movement-and-quantifier-raising} we discussed extensions of
the syntactic calculus. However, \lamET, too, has been extended and
revisited many times. Many of its extensions were created to deal with
complex semantic phenomena, such as intensionality~\citep{winter2009},
expressives~\citep{potts2003,mccready2010,gutzmann2011}, and dynamic
semantics~\citep{groenendijk1995}.

In \citeyear{shan2002}, \citeauthor{shan2002} proposed an interesting
paradigm to unify these extensions: by implementing them using
techniques for effectful functional programming in \lamET.

\citeauthor{shan2002} proposed to analyse a wide range of linguistic
phenomena using monads. He defines several monads which deal with
interrogatives, focus, intensionality, binding, and
quantification. \citet{bumford2013}, \citet{charlow2014} and
\citet{barker2015} continued this line of research, defining monads to
deal with a large range of linguistic phenomena.

Formally, a monad is
\begin{enumerate*}[label=(\arabic*)]
\item a type-level constructor, $\mathbb{M}$, mapping each type A
  to a corresponding type $\mathbb{M}A$; and
\item a pair of functions, η and $\star$ (pronounced ``unit'' and
  ``bind''), with the following types\footnote{
    In addition, these functions have to obey the monad laws: left
    identity ($M\star\eta N\equiv M\;N$); right identity
    ($\eta\star M\equiv M$); and associativity ($M\star(\lambda
    x.N\;x\star O) \equiv (M\star\lambda x.N\;x)\star O$).
  }:
\end{enumerate*}
\[
  η:A\ra\mathbb{M}A
  \qquad
  \star:(A\ra\mathbb{M}B)\ra\mathbb{M}A\ra\mathbb{M}B
\]
There are many ways to implement monadic semantics. The most
conventional of these is to apply the monadic translation, as
described by \citet{moggi1991}:
\[
  \begin{aligned}
    &(A\ra B)^\text{M} &&= A^M\ra\mathbb{M}B^M\\
    &A^\text{M}        &&= A
  \end{aligned}
  \qquad
  \begin{aligned}
    &\text{lift}\;x           &&= \eta\;x\\
    &\text{lift}(\lambda x.M) &&= \eta(\lambda x.\text{lift}\;M)\\
    &\text{lift}(M\;N)        &&= (\lambda f.f\star(\text{lift}\;N))\star\text{lift}\;M
  \end{aligned}
\]
Another possibility is to modify our translation to semantic calculus
to insert the monadic operators. If we choose to do this, we can use
the information present in out syntactic calculus to guide our
translation. For instance, we could simply choose to modify our
translation on types to insert an `$\mathbb{M}$' over every atomic
type:
\[
  \tr[\S]=\mathbb{M}\t
  \quad
  \tr[\N]=\mathbb{M}(\e\ra\t)
  \quad
  \tr[\NP]=\mathbb{M}\e
\]
Whichever choice we make, the important point is that the insertion of
the monad constructor $\mathbb{M}$ in our types gives us the
possibility to implement any sort of ``plumbing'' we need in our
lexical entries, as long as it forms a monad.

\vspace*{1\baselineskip}

As an example, we can use monads to analyse expressive content. This is
content that is present in the sentence meaning, but does not directly
affect the truth-conditional meaning. It is information present on a
sort-of side channel. For instance, in ``I walked the damn dog,'' the
word `damn' does not seem to contribute to the truth-conditional
meaning, as the utterance would still be considered truthful if the
dog is well-liked.

We can implement this using a variant of the writer monad: we
represent values of the type $\mathbb{M}A$ as a tuple of
truth-conditional (or ``at-issue'') content, and expressive content:
\begin{align*}
  \mathbb{M}A    &= A\times\t                                         \\
  \eta\;M        &= (M,\text{true})                                   \\
  M\star N       &= \case{M}{x}{y}{(\case{N\;x}{z}{w}{(z,y\land w)})} \\
  \text{tell}(M) &= ((),M)                                            \\
  \intertext{%
  Using this monad, we can define a small lexicon. We lift our regular
  entries into monadic entries:
  }
  \text{john}  &= \eta\;\JOHN\\
  \text{walks} &= \lambda y\;x.(\lambda x'.(\lambda y'.\eta(\WALK(x,y)))\star y)\star x\\
  \text{the}   &= \lambda f.(\lambda f'.\iota(f'))\star f\footnotemark\\
  \text{dog}   &= \eta\;\DOG\\
  \intertext{%
    We treat `damn' as an identity function in its at-issue
    content---it binds $x'$, then returns it. However, we also define
    `damn' as expressing some sort of displeasure, represented as the
    proposition $\DAMN$ in its expressive content:
  }
  \text{damn} &= \lambda f.(\lambda f'.(\lambda().\eta\;f')\star\text{tell}(\DAMN(f')))\star f
\end{align*}
\footnotetext{%
  The semantics for `the' are commonly given in terms of a function
  called `$\iota$', known as the ``definite description operator.''
  Given a set, this operator returns its unique inhabitant. Depending
  on the exact semantics you want for this operator, it can either be
  implemented monadically, as a side-effect, or using
  quantification. For this example, it is best to think of `$\iota$'
  as a function of the type $(\e\t)\e$.
}%
The entire utterance ``I walked the damn dog'' then reduces as follows:
\[
  (\text{walks}\;(\text{the}\;(\text{damn}\;\text{dog}))\;\text{john})
  \mapsto
  (\WALK(\JOHN,\iota(\DOG)), \DAMN(\DOG))
\]
The above analysis is rather coarse, as it does not capture any
displeasure towards the \emph{specific} dog. We \emph{can} get a more
precise meaning, but doing so complicates the example too much.

\vspace*{\baselineskip}

Monads have one big problem: modularity. There is no general procedure
which can compose two arbitrary monads $\mathbb{M}_1$ and
$\mathbb{M}_2$ into a new monad $\mathbb{M}_3 = \mathbb{M}_1 \circ
\mathbb{M}_2$. This means that it is not trivial to separate different
types of effects---\emph{all} side-effects have to be implemented in
one single, monolithic monad.

\citet{shan2002} mentions monad morphisms or transformers as a
solution to mitigate---but not solve---the problem.
Monad transformers were introduced by \citet{liang1995}. In short,
they are functions $\mathbb{T}$ from monads to monads. Transformers
can be chained together, to create combined monads consisting of
``layers'' of elementary monads. Because different monads combine in
different ways, the programmer has to manually define these
transformers, and has to to specify how effectful operations `lift'
through each monad transformer.
One problem with monad transformers is that the order of the
``layers'' is determined statically, and cannot easily be changed in
various parts of the program. In addition, every effectful operation
has to be lifted into this layercake of side-effects. This means that
that every effectful function, or lexical entry, has access to
\emph{all} side-effects, and every effectful function has to be
altered if a new layer is added. It is clear that monad transformers
offer a sub-par solution to the problem.\footnote{%
  The Haskell community is split over whether or not monad
  transformers are useful in practice, but many people---including the
  GHC developers---prefer ``rolling'' their own monolithic monad,
  which includes all required effects, over using monad transformers
  (see \url{http://stackoverflow.com/a/2760709}).
}

\citet{cartwright1994}, \citet{kiselyov2013} and \citet{kiselyov2015}
offer a solution to the problem of modularity, in the form of
\emph{extensible effects}. An in-depth discussion of extensible
effects is beyond the scope of this thesis, so we will simply give an
outline of the interface presented by the \citeyear{kiselyov2015}
implementation of extensible effects.

In short, this implementation a type constructor $\mathbb{E}_X$ which
is indexed by a ``set'' of effects. This constructor forms a monad for
arbitrary sets $X$, so we can keep using the style of lexical
definitions we saw above.
What sets extensible effects apart from monad transformers is that
with extensible effects, instead of defining a transformer and a
lifting operator, the programmer defines an `effect' in isolation from
every other effect. An effect is defined by three things:
\begin{enumerate*}[label=\arabic*)]
\item a type constructor, which links the type of effectful values to
  the type of the effect;
\item primitive effectful functions, such as `tell';
\item a handler, which removes the effect from the set of effects,
  optionally consuming or producing additional input or output.
\end{enumerate*}
In the case of expressive content, the effect is
defined as follows:\footnote{%
  The usage of $\top$ in the definition of Exp means that Exp is
  \emph{only} defined for the $\top$ type---this, in turn, forces the
  output type of `tell' to be $\top$.
}
\[
  \text{Exp}\;\top=\t
\]
Once we have this definition, we can recover the `tell' function using
one of the primitives offered by extensible effects---the `send'
function. In fact, `tell' is exactly the `send' function, with `$F$'
instantiated to the expressive effect:
\[
  \text{send} : FA\ra\mathbb{E}_{\{F\}\cup X}A
  \quad
  \textit{specialises to}
  \quad
  \text{tell} : \t\ra\mathbb{E}_{\{Exp\}\cup X}\top
\]
What we have gained at this point is the parameter $X$---the `tell'
function is now polymorphic in the set of effects. This means that a
word with only expressive content---such as `damn'---only has access
to the effects associated with expressive content, and not---as was
the case with monad transformers---to the entire stack of effects.
Last, we have to define a handler for the effect. This means defining
a function which takes a value which includes the effect, and returns
a value without it. For expressives, our handler will have the
following type:
\[
  \text{run}_{\text{Exp}} : \mathbb{E}_{\{Exp\}\cup X}A\ra\mathbb{E}_{X}(A\times\t)
\]
This handler will remove the expressive effect, and tuples the
expressive content with the at-issue content. In general, handlers
allow us to remove effects from the set of effects step-by-step until
we once again end up with an effect-free value.
