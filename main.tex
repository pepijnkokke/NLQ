\documentclass[a4paper]{article}

\input{preamble}

\begin{document}

% ``We are constructing a \emph{grammar logic}. Therefore, we only
% want features in our logic for which we can demonstrate a motivating
% example from natural language.''

%\include{introduction}
%\include{display-calculus}
%\include{lexical-ambiguity}

\section{Natural Language, Effects, and Movement}

In sections \autoref{sec:introduction} and
\autoref{sec:display-calculus} we presented a simple type-logical
grammar, and its encoding as a display calculus. In this section, we
will study extensions of the syntactic and semantic calculi, which
will allow us to analyse more linguistic phenomena.

Since its original formulation by \citet{lambek1961}, many variants of
the Lambek calculus have been proposed
\citep{steedman1988,moortgat2012,morrill2011,kubota2012,barker2015}.
However, none have yet become truly canonical.
Recently, \citet{moot2015} has begun comparing these extended calculi,
as fragments of first-order linear logic. In time, this may yield a
canonically agreed-upon extension to NL.

\lamET, too, has been extended and revisited many times. Many of these
extensions were created to deal with complex semantic phenomena, such
as intensionality~\citep{winter2009},
expressives~\citep{potts2003,mccready2010,gutzmann2011}, dynamic
semantics~\citep{groenendijk1995}, and quantifier
raising~\citep{barker2015}. \todo{Other important work?}
In recent research, \citet{shan2002} proposed an interesting paradigm
to unify these extensions: by using techniques for effectful
functional programming in \lamET.

Below, we will briefly discuss \citeauthor{shan2002}'s proposed
extensions to \lamET and its limitations. We will then turn our
attention to the extensions made to NL, and see how they can remedy
the limitations of the extended \lamET. We will conclude this section
by giving our own extensions to NL, and showing that they preserve the
properties we expect of a grammar logic (i.e. admissibility of cut,
and a decidable and complete proof search procedure).

\subsection{Monads and Extensible Effects}
In \citeyear{shan2002}, \citeauthor{shan2002} proposed to analyse a
wide range of linguistic phenomena using monads. He defines several
monads which deal with interrogatives, focus, intensionality, binding,
and quantification. \citet{bumford2013}, \citet{charlow2014} and
\citet{barker2015} continued this line of research, defining monads to
deal with a large range of linguistic phenomena.

Formally, a monad is \todo{Monad laws?}
\begin{enumerate*}[label=(\arabic*)]
\item a type-level constructor, $\mathbb{M}$, mapping each type A
  to a corresponding type $\mathbb{M}A$; and
\item a pair of functions, η and $\star$ (pronounced ``unit'' and
  ``bind''), with the following types:
\end{enumerate*}
\[
  η:A\ra\mathbb{M}A
  \qquad
  \star:\mathbb{M}A\ra(A\ra\mathbb{M}B)\ra\mathbb{M}B
\]
The idea of monadic semantics is make one small change the translation
from syntactic to semantic types, as defined in
\autoref{fig:syntactic-calculus-to-explicit-lamET}: instead of using
the direct mapping on atoms, we insert the monad constructor
$\mathbb{M}$. For instance,
\[
  \tr[\S]\mapsto\mathbb{M}\t
  \quad\text{,}\quad
  \tr[\N]\mapsto\mathbb{M}(\e\ra\t)
  \quad\text{and}\quad
  \tr[\NP]\mapsto\mathbb{M}\e
  \text{.}
\]
The term labelling remains the same. However, due to the insertion of
the monad constructor $\mathbb{M}$ in our types, we now have the
possibility to implement any sort of ``plumbing'' we need, as long as
it forms a monad.
\[
  \begin{aligned}
    \mathbb{M}A   &\coloneqq A\times\t       \\
    \eta(M)       &\coloneqq (M,\text{true}) \\
    M\star N      &\coloneqq \case{M}{x}{y}{(\case{N\;x}{z}{w}{(z,y\land w)})}
  \end{aligned}
\]
\[
  \begin{aligned}
    \text{john}   &\coloneqq \eta(\JOHN)\\
    \text{walks}  &\coloneqq \lambda y.\lambda x.
                       y\star(\lambda y'.x\star(\lambda x'.\eta(\WALK(x',y'))))\\
    \text{the}    &\coloneqq \lambda f.f\star(\lambda f'.\eta(\iota(f')))\\
    \text{damned} &\coloneqq \lambda x.x\star(\lambda x'.(x',\DAMN))\\
    \text{dog}    &\coloneqq \eta(\DOG)
  \end{aligned}
\]
\[
  (\text{walks}\;(\text{the}\;(\text{damned}\;\text{dog}))\;\text{john})
  \mapsto
  (\WALK(\JOHN,\iota(\DOG)), \DAMN)
\]


%\input{fig-extension-quantifier-raising}
%\input{fig-extension-scope-islands}
%\input{fig-extension-infixation}
%\input{fig-extension-extraction}

% - implicit semantic calculus;
% - explicit semantic calculus;
% - syntactic calculus;
% - display calculus;
% - compositionality principle;
% - problems with compositionality;
% - quantifier raising and scope ambiguity;
% - continuation monad & delimited continuations;
% - extension: lexical ambiguity;
% - extension: quantifier raising
%   * treatment of some & every;
%   * treatment of same & different;
%   * treatment of plurals;
% - extension: scope islands;

\section{Future work}

\paragraph*{Integrate Focusing and Display Logic}

\paragraph*{Deep Inference Sequent Calculus}
In \autoref{sec:why-use-display-calculus}, we discussed that in order
to use backward-chaining proof search, we have to ensure that our
structural rules have the sub-structure property, or at very least
only cause predictable loops. \citet{gore2014} demonstrate a
methodology for constructing a deep inference sequent calculus from a
display calculus. Deep inference calculi have the advantage that they
naturally have the sub-structure property, which means that they are
suitable for naive backward-chaining search. It would be interesting
to employ this methodology, and construct a deep inference sequent
calculus for the system constructed in this thesis.

\paragraph*{Forward-Chaining Proof Search}
In \autoref{sec:what-is-type-logical-grammar} it was mentioned that
most research focuses on implementing what I call the `semantic
function' (i.e.\ interpreting). This is a good approach for research:
we can limit ourselves to sequent calculus, which has pleasant
properties, refer to the huge body of work on generative grammar to
inform our choice for sentence structure, and simply focus on making
these known structures derivable. However, in order to be feasible in
a practical system, one must also implement what I call the `syntactic
function' (i.e.\ parsing).

One way to include parsing is by switching to forward-chaining proof
search. In principle, we can do this by constructing all possible
sentences based on the given words, and filtering on those which are
both pronounceable and maintain the correct word-order. Ideally,
though, we would also have an efficient implementation, for instance
one based on the technique of magic sets as developed by
\citet{bancilhon1985}.

\bibliographystyle{apalike}%
\bibliography{main}%

\end{document}
