\section{Display calculus and focused proof search}
\label{sec:display-calculus}

In this section, we will develop a display calculus
\citep{belnap1982} for NL. We will start out by presenting a display
calculus for NL based on work by \citet{moortgat2009},
\citet{bernardi2010}, \citet{moortgat2012} and \citet{gore1998}. We
will then continue by motivating our choice for display calculus over
sequent calculus.
In \autoref{sec:translation-to-lamET} we will relate our display
calculus back to the framework discussed in \autoref{sec:introduction},
by defining a translation from our display calculus back to \lamPET.
And finally, in \autoref{sec:focusing-and-spurious-ambiguity}, discuss
the problem of spurious ambiguity, and address this by developing an
extension to display calculus, using polarities and focusing
\citep{girard1991,bastenhof2011}, which is free of spurious
ambiguity.



\subsection{NL as a display calculus}
\label{sec:nl-as-a-display-calculus}
\input{fig-nl-display-calculus}

We present the display calculus for NL in
\autoref{fig:nl-display-calculus}.\footnote{%
  The calculus in \autoref{fig:nl-display-calculus} was first formulated
  by \citet{moortgat1999}---though without display calculus in
  mind. Because the logical and structural connectives in NL coincide,
  they did not think to distinguish between them. Therefore, they do
  not have the R$\impr$ and R$\impl$ rules, and their version has
  slightly different properties.
}
It features the same atoms and types as in
\autoref{fig:nl-natural-deduction}, but structures have been expanded:
there are now positive and negative structures---with one
structural connective for each logical connective---and residuation
rules to navigate them.\footnote{%
  For each connective, we use the \emph{same} symbol at both the
  logical and the structural level. This is unconventional. However,
  because each type, when used in a structure, is wrapped in the
  $\struct{\_}$-connective, this is perfectly unambiguous.
}
These two work together to guarantee the \emph{display
  property}---the property that any sub-structure can be made the sole
structure in either the antecedent or the succedent, depending on its
polarity. For instance, below we use residuation to isolate the object
NP on the left-hand side:\footnote{%
  Inverted applications of the residuation rules are marked by
  switching the operators---e.g. by writing Res$\prod\impr$ instead of
  Res$\impr\prod$.}%
\begin{pfblock}
  \AXC{$\vdots$}\noLine
  \UIC{$\struct{\NP}\prod(\struct{\TV}\prod\struct{\underline{\NP}})\fCenter\struct{\S}$}
  \RightLabel{Res$\prod\impr$}
  \UIC{$\struct{\TV}\prod\struct{\underline{\NP}}\fCenter\struct{\NP}\impr\struct{\S}$}
  \RightLabel{Res$\prod\impl$}
  \UIC{$\struct{\underline{\NP}}\fCenter(\struct{\NP}\impr\struct{\S})\impl\struct{\TV}$}
\end{pfblock}
Because the display property guarantees that any sub-structure can be
displayed on either the right- or the left-hand side of the turnstile,
we will occasionally abbreviate inferences such as the one above using
the display postulate (DP):\footnote{%
  We have attached a formal, executable proof of the display property
  for full NLQ---an extension of display NL as presented in this
  section. See the appendix A for more details.}
\begin{pfblock}
  \AXC{$\vdots$}\noLine
  \UIC{$\struct{\NP}\prod(\struct{\TV}\prod\struct{\underline{\NP}})\fCenter\struct{\S}$}
  \RightLabel{DP}
  \UIC{$\struct{\underline{\NP}}\fCenter(\struct{\NP}\impr\struct{\S})\impl\struct{\TV}$}
\end{pfblock}
Using the display property, we can trivially derive the sequent
calculus rules. For instance, below we derive the sequent-calculus
version of L$\impr$:\footnote{%
  $\Sigma'$ is the representation of $\Sigma$ after it has been moved
  by the display postulate. A formal definition of this relation is
  given in appendix A.
}
\begin{pfblock}
  \AXC{$\Sigma[\struct{B}]\fCenter{\struct{C}}$}
  \RightLabel{DP}
  \UIC{$\struct{B}\fCenter{\Sigma'[\struct{C}]}$}
  \AXC{$\Gamma\fCenter\struct{A}$}
  \BIC{$\struct{A\impr{B}}\fCenter\Gamma\impr\Sigma'[\struct{C}]$}
  \RightLabel{Res$\impr\prod$}
  \UIC{$\Gamma\prod\struct{A\impr{B}}\fCenter\Sigma'[\struct{C}]$}
  \RightLabel{DP}
  \UIC{$\Sigma[\Gamma\prod\struct{A\impr{B}}]\fCenter\struct{C}$}
\end{pfblock}

\vspace*{1\baselineskip}

Again, we use the product-free version of NL. Should the reader need a
product, though, it is easily added \citep[see][]{moortgat2012}:
\begin{center}
  \begin{pfbox}
    \AXC{$\struct{A}\prod\struct{B}\fCenter{X}$}
    \RightLabel{L$\otimes$}
    \UIC{$\struct{A\otimes{B}}\fCenter{X}$}
  \end{pfbox}
  \begin{pfbox}
    \AXC{$X\fCenter\struct{A}$}
    \AXC{$Y\fCenter\struct{B}$}
    \RightLabel{R$\otimes$}
    \BIC{$X\prod{Y}\fCenter\struct{A\otimes{B}}$}
  \end{pfbox}
\end{center}

One change that we made, aside from moving to display calculus, is
that the axiom has been restricted to atoms. This does not mean our
logic no longer has the identity, since it is derivable by simple
induction over the structure of the formula:
\begin{center}
  \begin{pfbox}
    \AXC{}
    \RightLabel{Ax}
    \UIC{$\struct{\alpha}\fCenter\struct{\alpha}$}
  \end{pfbox}
  \begin{pfbox}
    \AXC{$\vdots$}\noLine\UIC{$\struct{A}\fCenter\struct{A}$}
    \AXC{$\vdots$}\noLine\UIC{$\struct{B}\fCenter\struct{B}$}
    \RightLabel{L$\impr$}
    \BIC{$\struct{A\impr B}\fCenter\struct{A}\impr\struct{B}$}
    \RightLabel{R$\impr$}
    \UIC{$\struct{A\impr B}\fCenter\struct{A\impr B}$}
  \end{pfbox}
  \begin{pfbox}
    \AXC{$\vdots$}\noLine\UIC{$\struct{A}\fCenter\struct{A}$}
    \AXC{$\vdots$}\noLine\UIC{$\struct{B}\fCenter\struct{B}$}
    \RightLabel{L$\impr$}
    \BIC{$\struct{B\impl{A}}\fCenter\struct{B}\impl\struct{A}$}
    \RightLabel{R$\impl$}
    \UIC{$\struct{B\impl{A}}\fCenter\struct{B\impl{A}}$}
  \end{pfbox}
\end{center}
Instead, the change was made to avoid spurious ambiguity. If the
calculus \emph{were} to have a full identity, then there would be
e.g.\ \emph{two} proofs of the identity over \IV:
\begin{center}
  \begin{pfbox}
    \AXC{}
    \RightLabel{Ax}
    \UIC{$\struct{\NP\impr\S}\fCenter\struct{\NP\impr\S}$}
  \end{pfbox}
  \begin{pfbox}
    \AXC{}\RightLabel{Ax}\UIC{$\struct{\NP}\fCenter\struct{\NP}$}
    \AXC{}\RightLabel{Ax}\UIC{$\struct{\S}\fCenter\struct{\S}$}
    \RightLabel{L$\impr$}
    \BIC{$\struct{\NP\impr\S}\fCenter\struct{\NP}\impr\struct{\S}$}
    \RightLabel{R$\impr$}
    \UIC{$\struct{\NP\impr\S}\fCenter\struct{\NP\impr\S}$}
  \end{pfbox}
\end{center}
This is problematic. Generally, we only want to have two proofs for
the same sequent when that sequent is associated with an ambiguous
sentence. The derivation for e.g.\ ``Mary left'' contains the above
derivation---as $\NP\impr\S$ is the type of `left'---but this sentence
is not ambiguous at all!
In fact, when we use the derived identity described above, the two
proofs will expand to be equal. The problem of spurious ambiguity is
further discussed in \autoref{sec:focusing-and-spurious-ambiguity}.

In order for our calculus to be a valid display calculus, it needs to
obey eight simple conditions. Of these conditions, the only one that
involves any proof burden is \textbf{C8}---adapted from
\citet{gore1998}:
\begin{quote}
  If there are inference rules $ρ_1$ and $ρ_2$ with respective
  conclusions $Γ\fCenter\struct{A}$ and $\struct{A}\fCenter Δ$
  and if {Cut} is applied to yield $Γ\fCenter Δ$ then, either
  $Γ\fCenter Δ$ is identical to $Γ\fCenter\struct{A}$ or to
  $\struct{A}\fCenter Δ$; or it is possible to pass from the premises
  of $ρ_1$ and $ρ_2$ to $Γ\fCenter Δ$ by means of inferences falling
  under {Cut} where the cut-formula is always a proper sub-formula of
  $A$.
\end{quote}
In other words, we have to show that we can rewrite cuts on matching
left and right rules to smaller cuts on proper sub-formulas of the
cut-formula. For L$\impr$ and R$\impr$, this is done as follows:
\begin{center}
  \begin{pfbox}
    \AXC{$\vdots$}\noLine\UIC{$Π\fCenter\struct{A}\impr\struct{B}$}
    \RightLabel{R$\impr$}
    \UIC{$Π\fCenter\struct{A\impr B}$}
    \AXC{$\vdots$}\noLine\UIC{$Γ\fCenter\struct{A}$}
    \AXC{$\vdots$}\noLine\UIC{$\struct{B}\fCenter Δ$}
    \RightLabel{L$\impr$}
    \BIC{$\struct{A\impr B}\fCenter Γ\impr Δ$}
    \RightLabel{Cut}
    \BIC{$Π\fCenter Γ\impr Δ$}
  \end{pfbox}
  \\[1\baselineskip] $\Longrightarrow$ \\
  \begin{pfbox}
    \AXC{$\vdots$}\noLine\UIC{$Γ\fCenter\struct{A}$}
    \AXC{$\vdots$}\noLine\UIC{$Π\fCenter\struct{A}\impr\struct{B}$}
    \RightLabel{Res$\impr\prod$}
    \UIC{$\struct{A}\prodΠ\fCenter\struct{B}$}
    \AXC{$\vdots$}\noLine\UIC{$\struct{B}\fCenter Δ$}
    \RightLabel{Cut}
    \BIC{$\struct{A}\prod Π\fCenter Δ$}
    \RightLabel{Res$\prod\impl$}
    \UIC{$\struct{A}\fCenter Δ\impl Π$}
    \RightLabel{Cut}
    \BIC{$Γ\fCenter Δ\impl Π$}
    \RightLabel{Res$\impl\prod$}
    \UIC{$Γ\prod Π\fCenter Δ$}
    \RightLabel{Res$\prod\impr$}
    \UIC{$Π\fCenter Γ\impr Δ$}
  \end{pfbox}
\end{center}
And likewise for L$\impl$ and R$\impl$.
\note{\citet[][p.\ 83]{vanbenthem1995} discusses an additional
  constraint, namely that the ``trace'' in structural rules has to
  decrease. This discussion, however, relates to sequent
  calculus. Neither \citet{belnap1982} nor \citet{gore1998} discuss
  this property, which leads me to believe that this follows from the
  constraints on display calculus rules.
}


\subsection{Why use display calculus?}
\label{sec:why-use-display-calculus}
There are a few key advantages to using display calculus. First of
all, display calculus generalises sequent calculus. What this means is
that if something is a display calculus, it has all the properties
commonly associated with sequent calculus. Amongst others, display
calculus has the property that we are looking for: it has an easy to
implement, complete algorithm for proof search.

However, display calculus is more than sequent calculus. One of the
main theorems regarding sequent calculus---Gentzen's `Hauptsatz'---is
the proof of cut-elimination. Whereas for sequent calculus, this
theorem has to be proved separately for each instance, display
calculus has a generic proof of cut-elimination, which holds whenever
the calculus obeys certain easy to check conditions.

One last reason is that display calculus is, due to the way in which
it is usually formulated, relatively easy to formalise.

Below we will discuss these arguments in favour of display calculus in
more detail.

\paragraph{Practical proof search procedure}
All display calculi (by definition) have the sub-formula
property. However, in our display calculus, we have added structural
rules, which do not mention formulas. Therefore, we can no longer
guarantee termination---and thus decidability---based on the
sub-formula property alone.
Display calculi, however, do not necessarily have the
\emph{sub-structure} property---the property that a structural rule,
in its premises, can only use proper sub-structures of the structures
in its conclusion. This makes sense, since many logics depend
crucially on inference rules that do not have this property---e.g.\ %
contraction and weakening.
However, this does mean we will have to take special care that the
structural rules we introduce will not break the guarantee of
termination.

There is one simple extension we can make to the backward-chaining
proof search procedure, which gives us more freedom in the formulation
of structural rules. For this, let us have a look at the residuation
rules in \autoref{fig:nl-display-calculus}. They are crucial to the
display calculus, but clearly do not have the sub-structure property:
since they can be applied either way around, it is easy to see how
they can cause problems with termination.
This non-termination, however, is benign; these rules can only cause
\emph{loops}---any non-termination caused by them is guaranteed to
return to the same sequent.
We can extend our proof search procedure with loop-checking to cope
with this. We do this by
\begin{enumerate*}[label=(\arabic*)]
\item passing along a set of visited sequents;
\item stopping the proof search if we ever visit the same sequent
  twice; and
\item emptying out this set if we make progress---where progress means
  eliminating a connective.
\end{enumerate*}
Such an extension also preserves completeness, since any proof that
has a loop in it can be trivially rewritten to a proof without a loop by
cutting out the loop.

The problem then remains to avoid structural rules---or combinations
thereof---which can cause a divergence in which \emph{no} sequent is
visited more than once. We will discuss this further in
\autoref{sec:nl-lambda-nl-cl-and-nl-ibc}

\paragraph{Generic proof of cut-elimination}
Another important property of display calculus is the generic proof of
cut-elimination.
A proof of cut-elimination means that cut is an admissible rule, i.e.\
that every proof which uses the cut rule can be rewritten to a proof
that does not use the cut rule:
\begin{prooftree}
  \AXC{$Γ\fCenter A$}
  \AXC{$A\fCenter Δ$}
  \RightLabel{Cut}
  \BIC{$Γ\fCenter Δ$}
\end{prooftree}
This is important, amongst other reasons, because a logic has to admit
the cut rule by definition. However, if we were to include cut as an
explicit rule, we would no longer be able to use backward-chaining
proof search; the cut rule can always be applied, and introduces a
unknown formula $A$.

Another reason why cut is important is because it embodies a
linguistic intuition that many of us have: the idea that if you have a
sentence which contains a noun phrase---e.g.\ `a book' in ``Mary met
John''---and we have some other phrase of which we known that it is
also a noun phrase---e.g.\ ``the tallest man''---then we should be
able to substitute that second noun phrase for the first, and the
result should still be a grammatical sentence---e.g.\ ``Mary met the
tallest man.''

It should be clear that it is always important for the cut rule to be
admissible. However, in practice, one often has to give a separate
proof of cut-elimination for every logic. The generic proof of
cut-elimination for display calculus, however, states that if a
calculus obeys certain conditions \citep[see][]{gore1998}, the cut
rule is admissible.
This makes it an invaluable tool for research. In this thesis, we will
discuss several extensions to the non-associative Lambek calculus.
Because we know that each of these extensions respects the rules of
display calculus, we can be sure that any combination of then will
have a proof of cut-elimination, without having to prove this even
once.

\paragraph{Easy to formalise}
One last property of display calculus that is useful in formalising
the calculus, is the fact that display calculus does not rely on the
mechanisms of contexts and plugging functions, as used in
\autoref{fig:explicit-lamET} and the usual sequent
calculus formulation of NL.
These mechanisms are sometimes touted for simplifying the presentation
of proofs on paper, and for decreasing the complexity of proof
search---the idea being that there are fewer rules to apply.

However, they greatly complicate formal meta-logical proofs using, for
instance, proof assistants such as Coq or Agda.
For some intuition as to why, note that using contexts generally
inserts an application of the plugging function `\plug' in the
\emph{conclusions} of inference rules. This means that, in order to
do, for instance, a proof by induction on the structure of the
sequent, one has a much harder time proving which rules can lead to
this sequent.
In dependently-typed programming, the equivalent is inserting
function applications in the return types of the constructors of
datatypes. In their implementation of verified binary search trees,
\citet{mcbride2014} notes that this is bad design, as it leads to an
increased proof burden.

Another issue is that, on paper, researchers are used to reasoning
with a number of simplifications: the function `\plug' is often
overloaded to mean ``plug structure into context'' and ``compose two
contexts'', and we reason up to e.g.\ the equivalence between the two
($\Sigma_1[\Sigma_2[\Gamma]]\equiv (\Sigma_1[\Sigma_2])[\Gamma]$) and
the associativity of context composition. When using a proof
assistant, these implicit rewrite steps become painfully obvious.

To make matters worse, it is not trivial to see if these mechanisms
actually \emph{do} decrease the size of the proofs. Undoubtedly, there
are fewer rule applications, but the flipside of this is that each
rule application involving a context must now implicitly be decorated
with that context to be perfectly unambiguous.
Note that to apply a \emph{single} rule under a context of depth $d$,
we need to write $2d$ applications of residuation rules---$1d$ to move
down into the context, $1d$ to move back up---as opposed of annotating
with a single context of size $d$.\footnote{%
  Though contexts are usually conceptualised as ``structures with a
  single hole'', there is no need to write anything but the path to
  the hole---that is to say, it is perfectly unambiguous to write
  ``$\_\prodr(\square\prodl\_)$'' instead of e.g.\
  $\NP\prodr(\square\prodl((\N\impl\NP)\prod\NP))$. Therefore, we can
  say \emph{size} $d$ instead of \emph{depth} $d$.
}
A doubling in size. However, if we have $n$ successive applications
under the \emph{same} context, then residuation starts to take the
upper hand. For residuation, we still need to write $2d$ contexts, but
for annotated rules, we will need to repeat the context each time,
writing contexts of total size $nd$.

In a similar vein, it is hard to see whether these mechanisms reduce
amount of work to be done during proof search. While there are indeed
fewer rules, each of these rules can now be applied under a variety of
contexts.
This last point hints at another advantage of not using contexts: it
allows for the proof search algorithm to be truly trivial, as we can
say a rule applies if its conclusion can be unified with the current
proof obligation, and do not have to check all possible contexts under
which this unification could succeed.



\subsection{Terms for display NL}
\label{sec:translation-to-lamET}
In the previous sections, we defined a display calculus which is
equivalent to our natural deduction formulation of NL from
\autoref{sec:introduction}. However, there is still one thing missing
from our new implementation: terms.

We could translate display NL to natural deduction NL, and use the
term labelling that is the result of that translation. However, in
later sections we will extend display NL to be more expressive, and we
do not want to be forced to update the natural deduction formulation
as well. In addition, the extra indirection would complicate matters
too much. We therefore choose to give a direct translation from
display NL to lambda calculus.

\input{fig-extension-products-and-units}

There is one problem in translating display NL to the lambda
calculus: the structures for display NL are much more expressive. There
are structural connectives for implication, and since each logical
connective must have a structural equivalent, we will certainly add
more structural connectives in later sections. For this reason, we
choose to translate all structures to types. The one downside to this
is that we must translate the product `$\prod$' to the product type
`$\times$', and insert the necessary machinery to pack and unpack
these products.
In \autoref{fig:extension-products}, we extend our semantic calculus
with products. Anticipating future needs, we also extend it with
units.

The term labelling for display NL is presented in
\autoref{fig:nl-display-calculus-to-lamET}. The lambda terms are typed
by the translations of the formulas from display NL. As is usual when
translating sequent calculus to natural deduction, our term labelling
employs substitution, which was defined in
\autoref{sec:simple-type-logical-grammar}.
\note{When I look at \citet[][ch.\ 6]{capelletti2007}, I do not see a
  resemblance with what I am defining in this section. The only
  likeness I see is that we both write our label our syntactic rules
  with semantic terms. On the other hand, \citet[][ch.\
  6]{capelletti2007} does define spurious ambiguity w.r.t.\ the
  semantic calculus. Therefore, I have inserted a reference to his
  work in \autoref{sec:focusing-and-spurious-ambiguity}.
}

\input{fig-nl-display-calculus-to-lamET}

Now that we once again have a complete type-logical grammar, let us
take a quick look at an example, ``Mary likes Bill'':
\[
  \struct{\text{mary}:\NP}\prod(\struct{\text{likes}:\TV}\prod\struct{\text{bill}:\NP})\;
  \fCenter\;?:\struct{\S}
\]
If we search for proofs, using backward-chaining proof search, we find
the following proof:
\begin{pfblock}
  \AXC{}\RightLabel{Ax}\UIC{$\struct{\NP}\fCenter\struct{\NP}$}
  \AXC{}\RightLabel{Ax}\UIC{$\struct{\S}\fCenter\struct{\S}$}
  \RightLabel{L$\impr$}
  \BIC{$\struct{\NP\impr\S}\fCenter\struct{\NP}\impr\struct{\S}$}
  \AXC{}\RightLabel{Ax}\UIC{$\struct{\NP}\fCenter\struct{\NP}$}
  \RightLabel{L$\impl$}
  \BIC{$\struct{(\NP\impr\S)\impl\NP}\fCenter(\struct{\NP}\impr\struct{\S})\impl\struct{\NP}$}
  \RightLabel{Res$\impl\prod$}
  \UIC{$\struct{(\NP\impr\S)\impl\NP}\prod\struct{\NP}\fCenter\struct{\NP}\impr\struct{\S}$}
  \RightLabel{Res$\impr\prod$}
  \UIC{$\struct{\NP}\prod(\struct{(\NP\impr\S)\impl\NP}\prod\struct{\NP})\fCenter\struct{\S}$}
\end{pfblock}
Applying the translation from \autoref{fig:nl-display-calculus-to-lamET}
gives us the following lambda term:
\[
  s:\e\times(\e\e\t\times\e)\fCenter
  (\case{s}{\text{mary}}{(\text{likes},\text{bill})}
  {(\text{likes}\;\text{bill})\;\text{mary}}):\t
\]
This lambda term takes the sentence structure apart, and computes the
meaning. If this is desirable, it is possible to do some
post-processing with the structuralisation lemma:
\[
  \AX$A\hphantom{)}\fCenter B$
  \RightLabel{St}
  \UI$\text{St}(A)\fCenter B$
  \DisplayProof
  \quad
  \textbf{where}
  \quad
  \begin{aligned}
    &\text{St}(A\times B) &&\mapsto\text{St}(A),\text{St}(B)\\
    &\text{St}(\top)      &&\mapsto\emptyset                \\
    &\text{St}(A)         &&\mapsto A
  \end{aligned}
\]
This would result---after $\beta$-normalisation---in the following
lambda term:
\[
  \text{mary}:\NP,\text{likes}:\TV,\text{bill}:\NP
  \fCenter((\text{likes}\;\text{bill})\;\text{mary}):\t
\]
The lemma itself is fairly easy to derive by induction on the
antecedent. Using it has the advantage that the lambda term takes the
lexical definitions---the values for mary, likes and bill---from a
\emph{linear} structure, instead of from a nested tuple.




\subsection{Focusing and spurious ambiguity}
\label{sec:focusing-and-spurious-ambiguity}
In \autoref{sec:nl-as-a-display-calculus}, we briefly touched upon
spurious ambiguity. We investigated the spurious ambiguity inherent in
the axiom, but we never gave a formal definition of spurious
ambiguity.

To be able to give such a formal definition, we need the notion of a
\emph{normal-form}. For instance, \citet{andreoli1992} defines a
normal-form for full linear logic. This system removes e.g.\ the
choice in when to apply contraction and weakening, and in which order
to decompose the formulae. \citet{hepple1990} does a similar thing for
the Lambek calculus. Once we have a normal-form, we can define
spurious ambiguity in our search procedure as finding multiple proofs
for which the normal-forms are equal.

We can refine the above definition of spurious ambiguity, by referring
to the semantic interpretation \citep{capelletti2007,bastenhof2013}.
Below is a diagram representing the function which interprets
syntactic terms, where `$\tr$' is the translation from NL to \lamPET,
and `lex' is the function that inserts the lexical definitions for
words:\footnote{%
  Note that the second node is often considered to be \emph{linear}
  \lamET, i.e.\ the fragment of \lamET\ without contraction and
  weakening. This is because the syntactic calculus is not supposed
  to do copying or deletion, and we may want to limit non-linearity to
  lexical semantics.
}
\begin{center}
  \begin{tikzpicture}[center coordinate=(B)]
    \node             (A) {NL};
    \node[right=of A] (B) {$\lamET$};
    \node[right=of B] (C) {$\lamET$};

    \draw[->] (A) -- (B) node[draw=none, midway, above=1ex] {$\tr$};
    \draw[->] (B) -- (C) node[draw=none, midway, above=1ex] {lex};
  \end{tikzpicture}
\end{center}
We can define spurious ambiguity with respect to the second node,
i.e.\ the semantic terms after applying the translation `$\tr$'
\emph{and normalising}, but before inserting the lexical definitions.
In effect, we want a normal-form for NL which is guaranteed to
translate to a \emph{unique} normal-form term in \lamET.
\citet{moortgat2012} define such a normal-form for LG, for which we
present the NL fragment in \autoref{fig:nl-focused-display-calculus}.

\input{fig-nl-focused-display-calculus}

Our focused display calculus is a system with \emph{three} kinds of
sequents: the original structural sequent $Γ\fCenter{Δ}$, and two
focused sequents $\focus{A}\fCenter{Δ}$ and $Γ\fCenter\focus{B}$.
There are four new rules which communicate between these different
sequents: left and right focusing and unfocusing. In addition, the
axiom is split into a left- and a right-focused axiom.
The crucial point is that all formulas are assigned a polarity, and
that the axioms and the focusing and unfocusing rules are restricted to
formulas of certain polarities, forcing the proofs into a normal form.
For complex types, this polarity is based on the main connective, but
for atomic formulas, polarities are assigned. The choice of polarity
affects the kinds of ambiguity that we allow---this will be discussed
further in \autoref{sec:continuations-and-scope-taking}.\footnote{%
  For the remainder of this thesis, we will discuss extension in their
  focused form, and include the polarities of any new type. Should you
  wish to remove focusing, you can simply rewrite the focused sequents
  to structural sequents (i.e.\ $\focus{A}\equiv\struct{A}$), merge
  identifal rules, and remove any rules that become the identity.
}

Let us discuss an example. In the unfocused display calculus, there
are two proofs associated with the sentence ``Mary saw the fox'':
\begin{pfblock}
  \AXC{}\RightLabel{Ax}\UIC{$\struct\N \fCenter\struct\N$}
  \AXC{}\RightLabel{Ax}\UIC{$\struct\NP\fCenter\struct\NP$}
  \RightLabel{L$\impl$}
  \BIC{$\struct{\NP\impl\N}\fCenter\struct\NP\impl\struct\N$}
  \RightLabel{Res$\impl\prod$}
  \UIC{$\struct{\NP\impl\N}\prod\struct\N\fCenter\struct\NP$}
  \AXC{}\RightLabel{Ax}\UIC{$\struct\NP\fCenter\struct\NP$}
  \AXC{}\RightLabel{Ax}\UIC{$\struct\S \fCenter\struct\S$}
  \RightLabel{L$\impr$}
  \BIC{$\struct{\NP\impr\S}\fCenter\struct\NP\impl\struct\S$}
  \RightLabel{L$\impl$}
  \BIC{$\struct{(\NP\impr\S)\impl\NP}
    \fCenter(\struct\NP\impr\struct\S)\impl(\struct{\NP\impl\N}\prod\struct\N)$}
  \RightLabel{Res$\impl\prod$}
  \UIC{$\struct{(\NP\impr\S)\impl\NP}\prod\struct{\NP\impl\N}\prod\struct\N
    \fCenter\struct\NP\impr\struct\S$}
  \RightLabel{Res$\impr\prod$}
  \UIC{$\struct\NP\prod\struct{(\NP\impr\S)\impl\NP}
    \prod\struct{\NP\impl\N}\prod\struct\N\fCenter\struct\S$}
\end{pfblock}%
\begin{pfblock}
  \AXC{}\RightLabel{Ax}\UIC{$\struct\N \fCenter\struct\N$}
  \AXC{}\RightLabel{Ax}\UIC{$\struct\NP\fCenter\struct\NP$}
  \AXC{}\RightLabel{Ax}\UIC{$\struct\NP\fCenter\struct\NP$}
  \AXC{}\RightLabel{Ax}\UIC{$\struct\S \fCenter\struct\S$}
  \RightLabel{L$\impr$}
  \BIC{$\struct{\NP\impr\S}\fCenter\struct\NP\impr\struct\S$}
  \RightLabel{L$\impl$}
  \BIC{$\struct{(\NP\impr\S)\impl\NP}
    \fCenter(\struct\NP\impr\struct\S)\impl\struct\NP$}
  \RightLabel{Res$\prod\impl$}
  \UIC{$\struct{(\NP\impr\S)\impl\NP}\prod\struct\NP
    \fCenter(\struct\NP\impr\struct\S)$}
  \RightLabel{Res$\impr\prod$}
  \UIC{$\struct\NP
    \fCenter\struct{(\NP\impr\S)\impl\NP}\impr(\struct\NP\impr\struct\S)$}
  \RightLabel{L$\impl$}
  \BIC{$\struct{\NP\impl\N}\fCenter(\struct{(\NP\impr\S)\impl\NP}
    \impr(\struct\NP\impr\struct\S))\impl\struct\N$}
  \RightLabel{Res$\impl\prod$}
  \UIC{$\struct{\NP\impl\N}\prod\struct\N
    \fCenter\struct{(\NP\impr\S)\impl\NP}\impr(\struct\NP\impr\struct\S)$}
  \RightLabel{Res$\impr\prod$}
  \UIC{$\struct{(\NP\impr\S)\impl\NP}\prod\struct{\NP\impl\N}\prod\struct\N
    \fCenter\struct\NP\impr\struct\S$}
  \RightLabel{Res$\impr\prod$}
  \UIC{$\struct\NP\prod\struct{(\NP\impr\S)\impl\NP}
    \prod\struct{\NP\impl\N}\prod\struct\N\fCenter\struct\S$}
\end{pfblock}
These proofs are clearly distinct, but they are associated with the
\emph{same} function-argument structure, $(\text{saw}\; (\text{the}\;
\text{fox})\; \text{mary})$. This is problematic, since we only want
ambiguity in our syntax when there is ambiguity in the meaning.

However, if attempt to write these two proofs down in the focused
display calculus from \autoref{fig:nl-focused-display-calculus}, we
see that the first proof can easily be transcribed---inserting focusing
and unfocusing rules where needed---but the second proof is rejected,
as it contains a polarity error:
\begin{pfblock}
  \AXC{}
  \RightLabel{Ax$^L$}\UIC{$\focus \N \fCenter\struct\N$}
  \RightLabel{Foc$^L$}\UIC{$\struct\N \fCenter\struct\N$}
  \RightLabel{Unf$^R$}\UIC{$\struct\N \fCenter\focus \N$}
  \AXC{}
  \RightLabel{Ax$^L$}\UIC{$\focus \NP\fCenter\struct\NP$}
  \RightLabel{Foc$^L$}\UIC{$\struct\NP\fCenter\struct\NP$}
  \RightLabel{Unf$^R$}\UIC{$\struct\NP\fCenter\focus \NP$}
  \AXC{}
  \RightLabel{Ax$^L$}\UIC{$\focus \NP\fCenter\struct\NP$}
  \RightLabel{Foc$^L$}\UIC{$\struct\NP\fCenter\struct\NP$}
  \RightLabel{Unf$^R$}\UIC{$\struct\NP\fCenter\focus \NP$}

  \AXC{}
  \RightLabel{Ax $^L$}\UIC{$\focus \S \fCenter\struct\S$}

  \RightLabel{L$\impr$}
  \BIC{$\focus{\NP\impr\S}\fCenter\struct\NP\impr\struct\S$}
  \RightLabel{L$\impl$}
  \BIC{$\focus{(\NP\impr\S)\impl\NP}
    \fCenter(\struct\NP\impr\struct\S)\impl\struct\NP$}
  \RightLabel{Foc$^L$}
  \UIC{$\struct{(\NP\impr\S)\impl\NP}
    \fCenter(\struct\NP\impr\struct\S)\impl\struct\NP$}
  \RightLabel{Res$\prod\impl$}
  \UIC{$\struct{(\NP\impr\S)\impl\NP}\prod\struct\NP
    \fCenter(\struct\NP\impr\struct\S)$}
  \RightLabel{Res$\impr\prod$}
  \UIC{$\struct\NP
    \fCenter\struct{(\NP\impr\S)\impl\NP}\impr(\struct\NP\impr\struct\S)$}
  \RightLabel{Unf$^L$ $\la$ wrong}
  \UIC{$\focus\NP
    \fCenter\struct{(\NP\impr\S)\impl\NP}\impr(\struct\NP\impr\struct\S)$}
  \RightLabel{L$\impl$}
  \BIC{$\focus{\NP\impl\N}\fCenter(\struct{(\NP\impr\S)\impl\NP}
    \impr(\struct\NP\impr\struct\S))\impl\struct\N$}
  \RightLabel{Foc$^L$}
  \UIC{$\struct{\NP\impl\N}\fCenter(\struct{(\NP\impr\S)\impl\NP}
    \impr(\struct\NP\impr\struct\S))\impl\struct\N$}
  \RightLabel{Res$\impl\prod$}
  \UIC{$\struct{\NP\impl\N}\prod\struct\N
    \fCenter\struct{(\NP\impr\S)\impl\NP}\impr(\struct\NP\impr\struct\S)$}
  \RightLabel{Res$\impr\prod$}
  \UIC{$\struct{(\NP\impr\S)\impl\NP}\prod\struct{\NP\impl\N}\prod\struct\N
    \fCenter\struct\NP\impr\struct\S$}
  \RightLabel{Res$\impr\prod$}
  \UIC{$\struct\NP\prod\struct{(\NP\impr\S)\impl\NP}
    \prod\struct{\NP\impl\N}\prod\struct\N\fCenter\struct\S$}
\end{pfblock}
Had we chosen differently, and assigned \NP\ a positive polarity, this
proof would be valid. However, in that case the \emph{first} proof
would have contained a polarity error. We leave it as an exercise for
the reader to find this error.

Looking at the proof above---especially at the three left-most
axioms---one might wonder why we bother restricting the polarity for
axioms at all, since obviously the focusing and unfocusing rules allow
us to derive left- or right-focused sequents for any atom, regardless
of polarity. Once again, this is to avoid spurious ambiguity: if we
did not restrict the axioms, there would be two ways to derive either
axiom. The above proof has 4 axioms, so---if it were correct---that
would result in $2^4$ proofs.

Extending display NL with focusing raises one problem: does this
extension still obey the conditions for display calculus? We assume
that cut has the following form:
\vspace{-1\baselineskip}
\begin{pfblock}
  \AXC{$Γ\fCenter\focus{A}$}
  \AXC{$\focus{A}\fCenterΔ$}
  \RightLabel{Cut}
  \BIC{$Γ\fCenterΔ$}
\end{pfblock}
However, when we try to extend the proof for \textbf{C8}, we find
that---keeping in mind that we may want to add new connectives or
change the polarities of the atomic formulas---there are two places in
the proof where we cannot guarantee the polarity of certain formulas:
\begin{center}
  \vbox{
    \begin{pfbox}
      \AXC{$\vdots$}\noLine\UIC{$Π\fCenter\struct{A}\impr\struct{B}$}
      \RightLabel{R$\impr$}
      \UIC{$Π\fCenter\struct{A\impr B}$}
      \RightLabel{Unf$^R$}
      \UIC{$Π\fCenter\focus{A\impr B}$}
      \AXC{$\vdots$}\noLine\UIC{$Γ\fCenter\focus{A}$}
      \AXC{$\vdots$}\noLine\UIC{$\focus{B}\fCenter Δ$}
      \RightLabel{L$\impr$}
      \BIC{$\focus{A\impr B}\fCenter Γ\impr Δ$}
      \RightLabel{Cut}
      \BIC{$Π\fCenter Γ\impr Δ$}
    \end{pfbox}
    \\[1\baselineskip] $\Longrightarrow$ \\
    \begin{pfbox}
      \AXC{$\vdots$}\noLine\UIC{$Γ\fCenter\focus{A}$}
      \AXC{$\vdots$}\noLine\UIC{$Π\fCenter\struct{A}\impr\struct{B}$}
      \RightLabel{Res$\impr\prod$}
      \UIC{$\struct{A}\prodΠ\fCenter\struct{B}$}
      \RightLabel{Unf$^R$}
      \LeftLabel{wrong? $\ra$}
      \UIC{$\struct{A}\prodΠ\fCenter\focus{B}$}
      \AXC{$\vdots$}\noLine\UIC{$\focus{B}\fCenter Δ$}
      \RightLabel{Cut}
      \BIC{$\struct{A}\prod Π\fCenter Δ$}
      \RightLabel{Res$\prod\impl$}
      \UIC{$\struct{A}\fCenter Δ\impl Π$}
      \RightLabel{Unf$^L$ $\la$ wrong?}
      \UIC{$\focus{A}\fCenter Δ\impl Π$}
      \RightLabel{Cut}
      \BIC{$Γ\fCenter Δ\impl Π$}
      \RightLabel{Res$\impl\prod$}
      \UIC{$Γ\prod Π\fCenter Δ$}
      \RightLabel{Res$\prod\impr$}
      \UIC{$Π\fCenter Γ\impr Δ$}
    \end{pfbox}
  }
\end{center}
If we assume that we can cut either on an $\focus{A}$, or on an
$\struct{A}$, then we can construct proofs of \textbf{C8}; at the
places where we cannot guarantee the polarity of $A$ or $B$, we then
have a choice. For instance, in the position where we cut on $B$, if
$B$ is negative, we use the proof as written, but if $B$ is positive,
we use Unf$^L$ on the \emph{other} argument of cut, and cut on
$\struct{B}$. It is, however, uncertain if this will works within the
framework of display calculus, as it was formulated with a single
turnstile.
This means we can no longer be certain that focused NL is a display
calculus, which in turn means that we lose our generic proof of
cut-elimination.

For CNL, \citet{bastenhof2011} solves this problem by proving that
there is a normalisation procedure from display CNL to focused CNL.
Together with the trivial injection from focused CNL into display CNL,
we can show that the two are equivalent.
Thus, focused CNL is a display calculus by virtue of being equivalent
to display CNL. However, in order to define this procedure,
\citet{bastenhof2011} requires a cut-elimination procedure. As one of
our main motives for using display calculus is the generic proof of
the admissibility of cut, one can see the futility of this exercise.

Nonetheless, the work by \citeauthor{bastenhof2011} does assure us
that focused NL is indeed a display calculus, and indeed enjoys
an admissible cut.
If we make extensions to display NL beyond CNL, there is another body
of work we can lean on: \citet{andreoli1992} gives a cut-elimination
procedure for focused \emph{full linear logic}. Since any extension
that we propose in this thesis is less expressive than linear logic,
we believe that between these two results, anything we propose in this
thesis should have a valid focusing regime.\footnote{%
  Not accounting for any errors we may have made in formulating these
  regimes.
}

Because it does not fall within the scope of this thesis, we will
leave the problem of finding an elegant solution to integrating
focusing and display calculus as future work.
For the remainder of this thesis, we will \emph{assume} that focused
NLQ is complete with respect to display NLQ.
However, we will continue to give proofs of \citepos{belnap1982}
\textbf{C8} using display NLQ, so that---should we turn out to have
made an error our the definition of our focusing regime---the reader
can simply remove the focusing regime, and be left with a valid
display calculus.
